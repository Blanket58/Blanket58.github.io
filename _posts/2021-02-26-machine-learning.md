---
layout: post
title: 机器学习杂记
date: 2021-02-26
tag: Algorithms
katex: true
---

## 杂记

- 机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。
- 机器学习所研究的内容，是关于在计算机上从数据中产生“模型”的算法，即“学习算法”。
- 学得模型适用于新样本的能力，称为泛化能力。
- 归纳和演绎是科学推理的两大基本手段。前者是从特殊到一般的泛化过程，即从具体事实归结出一般性规律；后者是从一般到特殊的特化过程，即从基础原理推演出具体状况。“从样本中学习”显然是一个归纳过程，因此亦称为“归纳学习”。归纳学习有广义与狭义之分，广义大体相当于从样本中学习；狭义则要求从样本中学得概念，因此也称为“概念学习”或“概念形成”。但当下现实中常用的技术大多是黑箱模型。
- 任何一个有效的机器学习算法必然有其归纳偏好，否则如果假设空间中存在多个假设都与训练集一致，算法无法得出固定的结论。
- 存在一个一般性的原则来引导算法确立“正确的”偏好，“奥卡姆”剃刀是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个”。
- 机器学习分为两类：
  - 符号主义：包括决策树和基于逻辑的学习，能产生明确的概念
  - 连接主义：神经网络，模型是一个黑箱

## 概念

### 凸集

实数$\mathbb{R}$或复数$\mathbb{C}$向量空间中，集合$S$称为凸集，当且仅当$S$中任意两点的连线上的点也在集合$S$内，因为只有这样，集合的外形才是凸的，没有凹进去的部分，才叫做凸集。

### 距离

对于点$X(x_1, x_2)$和点$Y(y_1, y_2)$

- 欧氏距离

$$
d_{X, Y} = \sqrt {(x_1 - y_1) ^ 2 + (x_2 - y_2) ^ 2}
$$

- 曼哈顿距离

$$
d_{X, Y} = \vert x_1 - y_1 \vert + \vert x_2 - y_2 \vert
$$

- 闵可夫斯基距离

$$
d_{X, Y} = \sqrt[p] {(x_1 - y_1) ^ p + (x_2 - y_2) ^ p}
$$

- 余弦相似度

$$
\cos \theta = \frac {x_1y_1 + x_2y_2} {\sqrt {x_1^2 + x_2^2} \sqrt {y_1^2 + y_2^2}}
$$

余弦相似度考察两个向量在空间中方向的接近程度，若$\cos \theta$很小，则可以将两个点视为一个聚类簇中。

