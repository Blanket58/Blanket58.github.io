---
layout: post
title: 分类
date: 2021-09-25
tag: Algorithms
katex: true
---

## 决策树

1. 有训练集$D = \{X_1, ..., X_m\}$
2. 将训练集D的各个属性可能取值划分为不重合的属性集A
3. 开始循环
4. 从属性集A中选择最优属性做划分
5. 产生分支
6. 直至没有样本、或样本在所有属性取值一样、或样本属于同一类别停止

选择最优划分属性这一步很关键，分类树怎么选择（为了使分类错误率尽可能低）从两个方法入手：

- 基尼系数

$$
G = \sum_{k=1}^K \hat{p}_{mk}(1 - \hat{p}_{mk})
$$

其中$\hat{p}_{mk}$表示第m个区域的训练集中第k类所占比例，基尼系数越小结点越纯。

- 信息熵

$$
Ent(D) = - \sum_{k=1}^K \hat{p}_{mk} \log_2{\hat{p}_{mk}}
$$

信息熵同样越小结点越纯。

对于回归树，

