---
layout: post
title: 分类
date: 2021-09-25
tag: Algorithms
katex: true
---

## 决策树

### 原理

1. 有训练集$D = \{X_1, ..., X_m\}$
2. 将训练集D的各个属性可能取值划分为不重合的属性集A
3. 开始循环
4. 从属性集A中选择最优属性做划分
5. 产生分支
6. 直至没有样本、或样本在所有属性取值一样、或样本属于同一类别停止

### 划分属性

选择最优划分属性的这一步很关键。

#### 分类树

分类树目的是使分类错误率尽可能低，即此区域训练集中非最常见类所占比例最低。

$$
E = 1 - \max_{k} (\hat{p}_{mk})
$$

其中$\hat{p}_{mk}$表示第m个区域的训练集中第k类所占比例，从两个方法入手：

- 基尼系数

$$
G = \sum_{k=1}^K \hat{p}_{mk}(1 - \hat{p}_{mk})
$$

基尼系数越小结点越纯。

- 信息熵

$$
Ent(D) = - \sum_{k=1}^K \hat{p}_{mk} \log_2{\hat{p}_{mk}}
$$

同样信息熵越小结点越纯。

#### 回归树

对于回归树，可以通过使RSS最小做划分，划分后结点上的取值为其中所有训练集的平均值。

### 剪枝

可以通过代价复杂性剪枝来降低过拟合，对于回归树有如下的公式：

$$
\sum_{m=1}^{\vert T \vert} \sum_{x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha \vert T \vert
$$

其中公式加号左边的部分为总误差平方和RSS，右边$\alpha$为调整系数，$\vert T \vert$为总结点数。

#### 剪枝方式

- 预剪枝：在决策树生成过程中，对每个结点在划分前估计，若当前划分不能带来明显的信息增益，则标记为叶结点
- 后剪枝：先生成一棵大树，由底向上，判断若当前非叶结点标记为叶结点，分类错误率是否减少

#### 信息增益

若属性$A$有V个可能取值$\{A_1, ..., A_V\}$，用属性$A$划分会产生$V$个分支结点，这$V$个分支结点的总信息熵为：

$$
\sum_{v=1}^{V} \frac{\vert D_v \vert}{\vert D \vert} Ent(D_v)
$$

于是使用属性$A$进行划分的信息增益为：

$$
Ent(D) - \sum_{v=1}^{V} \frac{\vert D_v \vert}{\vert D \vert} Ent(D_v)
$$

其中$\vert D_v \vert / \vert D \vert$为权重项，即样本越多的结点上权重越大。

## 装袋法

Bagging，又称自助法聚集。因为决策树是高方差的，将训练集分成两部分分别生成的树一定是不同的。因此采用自助法，从原始数据集中抽样N次，生成N个不同的树，对于一个给定的测试样本，记录N棵树各自给出的结果，分类树取出现次数最多的类别，回归树取所有结果的平均。

## 随机森林

与bagging类似，不同的是，在每一次分支结点时，候选属性为从原始属性集中抽样出的子集。因为当许多属性相关时，取较小的子集的效果较好，这称之为对树去相关。

## Boosting

不像直接生成一颗大树那样在每个结点都尝试向下分，提升法会在上一颗树的分类错误率基础上试图构建一颗新树使得分类错误率得到改善。

