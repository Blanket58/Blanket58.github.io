---
layout: post
title: 聚类
date: 2021-09-24
tag: Algorithms
katex: true
---

## K-means

**原理：**

1. 首先随机从样本集中找K个点当作K个聚类簇的均值点
2. 计算所有样本点分别与各个均值点的距离，将样本点归入距离最小的簇中
3. 重新计算每个聚类簇的均值点位置
4. 重复操作，直至达到指定迭代次数、或临近两次迭代均值点的Frobenius范数变动小于阈值

> 其中Frobenius范数也称欧几里得范数，即矩阵中每个元素的平方和再开方。在这里矩阵指的是K行n列的质心矩阵，它的每一行为一个质心，每一列为该质心在各个维度上的坐标。
>

**优点：**

- 原理简单、实现容易、收敛速度快（时间复杂度O(nkq)，其中n表示样本数，k表示类别数，q表示迭代次数）；
- 聚类效果较优；
- 算法的可解释性比较强；
- 主要需要调参的参数仅仅是簇数K。

**缺点：**

- K值的选取不好把握；
- 对于不是[凸的数据集](/2021/02/machine-learning/)比较难收敛；
- 如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳；
- 采用迭代方法，得到的结果只是局部最优；
- 只适用于连续型变量，因为如果有类别型数据，如性别这样的变量，最后聚类的质心若停留在(0, 1)内，是不合理无法解释的；
- 它的决策边界一定是线性的，对于线性不可分的数据算法会失效，但可以通过使用一个核函数将数据向高维映射，使得数据在高维空间中变得线性可分。

**注意：**

- K可以根据手肘图来确定，找到残差平方和下降速率变慢的那个拐点

$$
SSE = \sum_{k=1}^{K} \sum_{p \in C_i} \Vert p - m_i \Vert ^ 2
$$

其中p为数据对象，$m_i$是簇$C_i$的平均值，SSE一定会收敛，最后质心不再变化。

- 所有变量单位一定要一致，min-max标准化或z-score标准化。
- 因为受初始点影响较大，一般多做几次训练，最后选择多次结果的均值。
- 计算样本点与簇的距离时，可选最短、最长或与质心的距离，并且也不止欧氏距离一种[算法](/2021/02/machine-learning/)。

## 层次聚类

**原理：**

试图在不同层次上对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合策略或“自顶向下”的拆分策略。聚合策略的过程如下：

1. 首先将样本集中的每个点都视为一个聚类簇
2. 计算各个聚类簇之间的距离，将距离最短的两个簇合并
3. 重复第二步操作，直至剩余K个簇

**优点：**

- 距离和规则的相似度容易定义，限制少；
- 不需要预先给定聚类簇数；
- 可以发现类的层次关系，用于生成概念、文档层次树等；
- 簇的大小可以不同；
- 可以聚类成任意形状。

**缺点：**

- 算法时间复杂度较大；
- 离群点能产生很大影响。

## DBSCAN

**原理：**

首先定义四个概念：

- 核心对象：在A的$\varepsilon$邻域内，至少有N个样本
- 密度直达：B在A的$\varepsilon$邻域内，且A为核心对象，于是B由A密度直达
- 密度可达：C由B密度直达，于是C由A密度可达
- 密度相连：D由A密度直达，则C与D密度相连

达到最大长度的密度相连簇个数即总簇数，每个簇由密度相连的核心对象与它们$\varepsilon$邻域内的样本点组成。

**优点：**

- 可以处理任何形状的聚类簇，能够检测异常点。

**缺点：**

- 需要给定数据点的邻域半径$\varepsilon$和最少样本数量N，对输入参数较敏感。

